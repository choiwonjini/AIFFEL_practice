{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOClUfpF19GYX+SSRI+c020"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["데이터세트, 데이터로더를 활용한 다중선형회귀 구현"],"metadata":{"id":"j7fRq-MDR_ws"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import TensorDataset, DataLoader"],"metadata":{"id":"JpL5-6aHSLhr","executionInfo":{"status":"ok","timestamp":1760021235606,"user_tz":-540,"elapsed":4251,"user":{"displayName":"최원진","userId":"05911164394888239170"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# 데이터 생성\n","train_x = torch.FloatTensor([\n","    [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]\n","])\n","train_y = torch.FloatTensor([\n","    [0.1, 1.5], [1, 2.8], [1.9, 4.1], [2.8, 5.4], [3.7, 6.7], [4.6, 8]\n","])"],"metadata":{"id":"x7iikjk8SZyv","executionInfo":{"status":"ok","timestamp":1760021235654,"user_tz":-540,"elapsed":42,"user":{"displayName":"최원진","userId":"05911164394888239170"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# 텐서 데이터세트를 활용해 훈련용 데이터세트 생성\n","train_dataset = TensorDataset(train_x, train_y)\n","train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=True)"],"metadata":{"id":"JF4roFLESi3l","executionInfo":{"status":"ok","timestamp":1760021235659,"user_tz":-540,"elapsed":2,"user":{"displayName":"최원진","userId":"05911164394888239170"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["batch_size=2: 한 번의 배치마다 두 개의 데이터 샘플과 정답을 가져오게 함.  \n","shuffle=True: 불러오는 데이터의 순서를 무작위로 설정  \n","drop_last=True: 배치 크기에 맞지 않는 배치를 제거  \n","-> 전체 데이터세트의 크기가 5일 때, 배치 크기가 2라면 마지막 배치의 크기는 1이 된다.  \n","-> 배치 크기로 설정한 2보다 작으므로 마지막 배치를 학습에 사용하지 않겠다는 것이다."],"metadata":{"id":"UF3yy47CTCC5"}},{"cell_type":"code","source":["# 모델, Loss, 옵티마이저 선언\n","model = nn.Linear(2, 2, bias=True)\n","criterion = nn.MSELoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001)"],"metadata":{"id":"eEiDb5eKTmjE","executionInfo":{"status":"ok","timestamp":1760021241283,"user_tz":-540,"elapsed":5623,"user":{"displayName":"최원진","userId":"05911164394888239170"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["train_x, train_y 모두 (n, 2)의 크기를 가지므로 입/출력 데이터 차원 크기는 모두 2가 된다."],"metadata":{"id":"tHsP-dL8YQN1"}},{"cell_type":"code","source":["for epoch in range(20000):\n","    cost = 0.0\n","\n","    # 미니 배치 단위로 학습\n","    for batch in train_dataloader:\n","        x, y = batch # 데이터 분리\n","        output = model(x) # 예측\n","\n","        loss = criterion(output, y) # 손실 계산\n","\n","        optimizer.zero_grad() # 이전 그래디언트 초기화\n","        loss.backward() # 역전파를 통한 그래디언트 계산\n","        optimizer.step() # 파라미터 업데이트\n","\n","        cost += loss # 배치 손실 누적\n","\n","    # 에포크 당 평균 손실 계산\n","    cost = cost / len(train_dataloader)\n","\n","    # 1000 에포크마다 로그 출력\n","    if (epoch + 1) % 1000 == 0:\n","        print(f\"Epoch : {epoch+1:4d}, Model : {list(model.parameters())}, Cost : {cost:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q57iI08SYf8U","executionInfo":{"status":"ok","timestamp":1760021272456,"user_tz":-540,"elapsed":31155,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"11d56579-6b68-473c-ca18-953c22c6a9e1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch : 1000, Model : [Parameter containing:\n","tensor([[0.4803, 0.3181],\n","        [0.8498, 0.3685]], requires_grad=True), Parameter containing:\n","tensor([-0.6911,  0.1746], requires_grad=True)], Cost : 0.030\n","Epoch : 2000, Model : [Parameter containing:\n","tensor([[0.5833, 0.2649],\n","        [0.9326, 0.3258]], requires_grad=True), Parameter containing:\n","tensor([-0.8474,  0.0491], requires_grad=True)], Cost : 0.008\n","Epoch : 3000, Model : [Parameter containing:\n","tensor([[0.6358, 0.2378],\n","        [0.9748, 0.3040]], requires_grad=True), Parameter containing:\n","tensor([-0.9270, -0.0149], requires_grad=True)], Cost : 0.002\n","Epoch : 4000, Model : [Parameter containing:\n","tensor([[0.6626, 0.2240],\n","        [0.9963, 0.2929]], requires_grad=True), Parameter containing:\n","tensor([-0.9675, -0.0475], requires_grad=True)], Cost : 0.001\n","Epoch : 5000, Model : [Parameter containing:\n","tensor([[0.6762, 0.2170],\n","        [1.0073, 0.2872]], requires_grad=True), Parameter containing:\n","tensor([-0.9882, -0.0641], requires_grad=True)], Cost : 0.000\n","Epoch : 6000, Model : [Parameter containing:\n","tensor([[0.6831, 0.2134],\n","        [1.0129, 0.2843]], requires_grad=True), Parameter containing:\n","tensor([-0.9987, -0.0726], requires_grad=True)], Cost : 0.000\n","Epoch : 7000, Model : [Parameter containing:\n","tensor([[0.6867, 0.2115],\n","        [1.0157, 0.2829]], requires_grad=True), Parameter containing:\n","tensor([-1.0041, -0.0769], requires_grad=True)], Cost : 0.000\n","Epoch : 8000, Model : [Parameter containing:\n","tensor([[0.6885, 0.2106],\n","        [1.0171, 0.2821]], requires_grad=True), Parameter containing:\n","tensor([-1.0068, -0.0791], requires_grad=True)], Cost : 0.000\n","Epoch : 9000, Model : [Parameter containing:\n","tensor([[0.6894, 0.2101],\n","        [1.0179, 0.2817]], requires_grad=True), Parameter containing:\n","tensor([-1.0082, -0.0802], requires_grad=True)], Cost : 0.000\n","Epoch : 10000, Model : [Parameter containing:\n","tensor([[0.6899, 0.2099],\n","        [1.0183, 0.2816]], requires_grad=True), Parameter containing:\n","tensor([-1.0089, -0.0808], requires_grad=True)], Cost : 0.000\n","Epoch : 11000, Model : [Parameter containing:\n","tensor([[0.6901, 0.2098],\n","        [1.0184, 0.2815]], requires_grad=True), Parameter containing:\n","tensor([-1.0093, -0.0810], requires_grad=True)], Cost : 0.000\n","Epoch : 12000, Model : [Parameter containing:\n","tensor([[0.6902, 0.2097],\n","        [1.0185, 0.2814]], requires_grad=True), Parameter containing:\n","tensor([-1.0094, -0.0812], requires_grad=True)], Cost : 0.000\n","Epoch : 13000, Model : [Parameter containing:\n","tensor([[0.6903, 0.2097],\n","        [1.0186, 0.2814]], requires_grad=True), Parameter containing:\n","tensor([-1.0095, -0.0813], requires_grad=True)], Cost : 0.000\n","Epoch : 14000, Model : [Parameter containing:\n","tensor([[0.6903, 0.2097],\n","        [1.0186, 0.2814]], requires_grad=True), Parameter containing:\n","tensor([-1.0096, -0.0813], requires_grad=True)], Cost : 0.000\n","Epoch : 15000, Model : [Parameter containing:\n","tensor([[0.6903, 0.2096],\n","        [1.0186, 0.2814]], requires_grad=True), Parameter containing:\n","tensor([-1.0096, -0.0813], requires_grad=True)], Cost : 0.000\n","Epoch : 16000, Model : [Parameter containing:\n","tensor([[0.6903, 0.2096],\n","        [1.0186, 0.2814]], requires_grad=True), Parameter containing:\n","tensor([-1.0096, -0.0813], requires_grad=True)], Cost : 0.000\n","Epoch : 17000, Model : [Parameter containing:\n","tensor([[0.6904, 0.2096],\n","        [1.0186, 0.2814]], requires_grad=True), Parameter containing:\n","tensor([-1.0096, -0.0813], requires_grad=True)], Cost : 0.000\n","Epoch : 18000, Model : [Parameter containing:\n","tensor([[0.6904, 0.2096],\n","        [1.0186, 0.2814]], requires_grad=True), Parameter containing:\n","tensor([-1.0096, -0.0814], requires_grad=True)], Cost : 0.000\n","Epoch : 19000, Model : [Parameter containing:\n","tensor([[0.6904, 0.2096],\n","        [1.0186, 0.2814]], requires_grad=True), Parameter containing:\n","tensor([-1.0096, -0.0814], requires_grad=True)], Cost : 0.000\n","Epoch : 20000, Model : [Parameter containing:\n","tensor([[0.6904, 0.2096],\n","        [1.0186, 0.2814]], requires_grad=True), Parameter containing:\n","tensor([-1.0096, -0.0814], requires_grad=True)], Cost : 0.000\n"]}]},{"cell_type":"markdown","source":["### 📖 TensorDataset: 데이터 포장하기\n","TensorDataset은 여러 텐서를 마치 하나의 **'묶음 상품'**처럼 만들어주는 역할을 합니다.  \n","\n","핵심 역할: 입력 데이터(Features)와 정답 데이터(Labels)를 한 쌍으로 묶어줍니다.  \n","\n","동작 방식: 인덱스(index)를 통해 특정 위치의 데이터 쌍([입력, 정답])에 접근할 수 있게 합니다.  \n","\n","예시: my_dataset[0] → 첫 번째 [입력, 정답] 쌍을 반환  \n","\n","비유: 여러 재료(입력, 정답)를 하나의 밀키트(Meal Kit) 안에 가지런히 담아두는 것과 같습니다.  \n","\n","### 👨‍🍳 DataLoader: 데이터 공급하기\n","DataLoader는 포장된 데이터셋(TensorDataset)을 받아, 모델이 학습할 수 있도록 효율적으로 '서빙'해주는 역할을 합니다.  \n","\n","핵심 역할: 데이터셋을 미니배치(mini-batch) 단위로 나누어 전달합니다.  \n","\n","주요 기능:  \n","\n","batch_size: 한 번에 모델에 전달할 데이터의 양(묶음 크기)을 결정합니다.\n","\n","shuffle: 데이터 순서를 무작위로 섞어 모델의 과적합을 방지합니다.\n","\n","num_workers: 여러 프로세스를 사용해 데이터 로딩 속도를 높입니다.\n","\n","비유: 밀키트(TensorDataset)를 받아, 레시피에 맞게 정해진 양만큼 재료를 꺼내주는 요리사와 같습니다.\n","\n","### ⚙️ 둘의 관계: 학습 워크플로우\n","모델 학습 시 이 둘은 다음과 같은 순서로 함께 사용됩니다.\n","\n","데이터 준비:\n","입력(x)과 정답(y) 데이터를 torch.Tensor로 만듭니다.  \n","\n","포장 (TensorDataset):\n","dataset = TensorDataset(x, y)  \n","입력과 정답 텐서를 하나의 데이터셋으로 묶습니다.\n","\n","서빙 준비 (DataLoader):  \n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  \n","묶인 데이터셋을 32개씩, 순서를 섞어서 전달하도록 설정합니다.\n","\n","모델 학습:  \n","for inputs, labels in dataloader:  \n","DataLoader가 제공하는 미니배치를 사용하여 모델을 학습시킵니다."],"metadata":{"id":"E02mDp77ZRIY"}},{"cell_type":"code","source":[],"metadata":{"id":"kvgRsL6md4b3"},"execution_count":null,"outputs":[]}]}