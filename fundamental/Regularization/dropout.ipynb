{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218b8ec1-82a3-4c6f-b273-9aeb9d4900d5",
   "metadata": {},
   "source": [
    "# Drpoout 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4d13dd-99aa-4dd3-8581-da8bf0b64af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad4a85-baf6-4316-9dab-9c3484e3c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "X = train_dataset.data.numpy().astype(np.float32) / 255.0 \n",
    "y = train_dataset.targets.numpy().astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8dbf8-72f8-4064-ad2b-7f49331cfd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X).view(-1, 28*28)  # Flatten\n",
    "y_train = torch.from_numpy(y)\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90e29c-6086-438d-adbc-27535cbeafed",
   "metadata": {},
   "source": [
    "dropout의 확률을 0.9로 주었을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749a1a5-86a5-4706-8df5-a47e066fcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6750d11-b64f-471e-a029-a9f2cb942f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, criterion, optimizer, batch_size, epochs=5):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    num_samples = X_train.size(0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        permutation = torch.randperm(num_samples)  # 데이터 섞기 (Epoch마다)\n",
    "\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            idx = permutation[i:i+batch_size]\n",
    "            images, labels = X_train[idx].to(device), y_train[idx].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {total_loss:.4f}, Accuracy: {100. * correct / num_samples:.2f}%\")\n",
    "\n",
    "    print(f\"\\n⏱ Total Time: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7544d-d1db-4fe5-92b2-c47399f7092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_with_dropout = ClassifierModel(dropout_rate=0.9).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_with_dropout.parameters(), lr=0.001)\n",
    "\n",
    "train(model_with_dropout, X_train, y_train, criterion, optimizer, batch_size=batch_size, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab4ef4-2916-47b2-8316-8205faddff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_without_dropout = ClassifierModel(dropout_rate=0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_without_dropout.parameters(), lr=0.001)\n",
    "\n",
    "train(model_without_dropout, X_train, y_train, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3057d2b9-cc11-4ac0-93c7-1d037b017793",
   "metadata": {},
   "source": [
    "현재 이 데이터 셋은 학습이 잘 되는 데이터 셋으로, fully connected layer에서도 결과가 잘 나옴을 확인할 수 있는데요. 일부러 중간에 dropout layer를 추가하여 0.9의 확률 값을 주니 학습이 안 됨을 확인하였습니다. 다음은 overfitting이 나는 환경에서 dropout의 중요성을 알아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfbe1a-255f-4deb-8865-ad7131be7267",
   "metadata": {},
   "source": [
    "## 과적합 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdebfb-ee5b-4f6d-9597-eedb93c5c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.99 * len(X))\n",
    "valid_size = len(X) - train_size\n",
    "train_data, valid_data, train_label, valid_label = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "\n",
    "X_train = torch.from_numpy(train_data).reshape(-1, 28*28)\n",
    "y_train = torch.from_numpy(train_label)\n",
    "X_valid = torch.from_numpy(valid_data).reshape(-1, 28*28)\n",
    "y_valid = torch.from_numpy(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001898b-825e-43cc-b35c-11675b1fc68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverfitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049625b-5e7a-4c98-a9f8-f3119a158ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, criterion, optimizer, batch_size = batch_size, epochs=200):\n",
    "    train_loss, valid_loss, train_acc, valid_acc = [], [], [], []\n",
    "    model.train()\n",
    "    num_train = X_train.size(0)\n",
    "    num_valid = X_valid.size(0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        permutation = torch.randperm(num_train)\n",
    "        total_loss = 0\n",
    "        correct_train, total_train = 0, 0\n",
    "\n",
    "        for i in range(0, num_train, batch_size):\n",
    "            idx = permutation[i:i+batch_size]\n",
    "            images, labels = X_train[idx].to(device), y_train[idx].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss.append(total_loss / num_train // batch_size)\n",
    "        train_acc.append(100 * correct_train / total_train)\n",
    "\n",
    "        # === validation ===\n",
    "        model.eval()\n",
    "        temp_loss, correct_valid, total_valid = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, num_valid, batch_size):\n",
    "                images, labels = X_valid[i:i+batch_size].to(device), y_valid[i:i+batch_size].to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                temp_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += predicted.eq(labels).sum().item()\n",
    "\n",
    "        valid_loss.append(temp_loss / num_valid // batch_size)\n",
    "        valid_acc.append(100 * correct_valid / total_valid)\n",
    "        model.train()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss[-1]:.4f}, Valid Loss: {valid_loss[-1]:.4f}, Train Acc: {train_acc[-1]:.2f}%, Valid Acc: {valid_acc[-1]:.2f}%\")\n",
    "    return train_loss, valid_loss, train_acc, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16726eef-12f6-40b0-aaa6-1347ab8d68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dropout이 없는 모델 학습\n",
    "model_overfit = OverfitModel().to(device)\n",
    "optimizer = optim.Adam(model_overfit.parameters(), lr=0.001)\n",
    "train_loss, valid_loss, train_acc, valid_acc = train(model_overfit, X_train, y_train, X_valid, y_valid, criterion, optimizer, batch_size=64, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e011d-8cd8-481f-b439-e3156b9ed7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 값을 plot 해보겠습니다.\n",
    "plt.plot(train_loss, label='Train Loss', color='blue')\n",
    "plt.plot(valid_loss, label='Validation Loss', color='red')\n",
    "plt.legend()\n",
    "plt.title('Loss Graph without Dropout')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c0f01-aa27-4910-a3fe-14f10686d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 값을 plot 해보겠습니다.\n",
    "plt.plot(train_acc, label='Train Accuracy', color='blue')\n",
    "plt.plot(valid_acc, label='Validation Accuracy', color='red')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Graph without Dropout')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7f28d-fba6-435a-a106-ca948fa9519a",
   "metadata": {},
   "source": [
    "dropout layer가 없는 fully connected layer에서 200번 정도의 학습을 하니 train set의 accuracy는 올라가고, loss는 점점 떨어졌습니다. 그러나 validation set의 accuracy와 loss는 어느 정도 값에서 수렴함을 볼 수 있었습니다.\n",
    "이렇게 오버피팅을 만든 환경에서 dropout layer를 추가한 뒤 나머지 환경은 같게 한 실험을 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c075bf-31c6-4c94-b184-43a1d7a18537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropModel(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        # 여기에 dropout layer를 추가해보았습니다. 나머지 layer는 위의 실습과 같습니다.\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # 여기에 dropout layer를 추가해보았습니다. 나머지 layer는 위의 실습과 같습니다.\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466aa20-8949-4e98-9e11-0abab08e2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_dropout = DropModel(dropout_rate=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_dropout.parameters(), lr=0.001)\n",
    "train_loss, valid_loss, train_acc, valid_acc = train(model_dropout, X_train, y_train, X_valid, y_valid, criterion, optimizer, batch_size = batch_size, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bfe3f-6f99-4215-b42b-95ca94347dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. loss 값의 그래프를 그려봅시다.\n",
    "plt.plot(train_loss, label='Train Loss', color='blue')\n",
    "plt.plot(valid_loss, label='Validation Loss', color='red')\n",
    "plt.legend()\n",
    "plt.title('Loss Graph without Dropout')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0a1b0-6695-42c8-8108-362b6a443819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. accuracy 값의 그래프를 그려봅시다.\n",
    "plt.plot(train_acc, label='Train Accuracy', color='blue')\n",
    "plt.plot(valid_acc, label='Validation Accuracy', color='red')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Graph without Dropout')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bdc0f-9828-46d3-9e1d-93c993b37753",
   "metadata": {},
   "source": [
    "좋은 데이터를 가지고 오버피팅을 만드는 환경이 조금 어렵긴 했지만, dropout layer 하나만으로도 오버피팅을 막고, 두 데이터 셋이 정확도도 비슷하게 나옴을 확인하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e01ab-b5fa-4a29-8b4c-bdd5c3064a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
