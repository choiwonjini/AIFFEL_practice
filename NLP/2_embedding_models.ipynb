{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPMER/KeWrZwfVbZkJ3tokn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. N-gram"],"metadata":{"id":"r8HKSuHDJK0C"}},{"cell_type":"code","source":["import nltk\n","\n","def ngrams(sentence, n):\n","    words = sentence.split()\n","    ngrams = zip(*[words[i:] for i in range(n)])\n","    return list(ngrams)\n","\n","sentence = \"안녕하세요 만나서 진심으로 반가워요\"\n","\n","# 직접 구현한 ngram\n","unigram = ngrams(sentence, 1)\n","bigram = ngrams(sentence, 2)\n","trigram = ngrams(sentence, 3)\n","\n","print(unigram)\n","print(bigram)\n","print(trigram)\n","\n","# NLTK에서 지원하는 ngram\n","unigram = nltk.ngrams(sentence.split(), 1)\n","bigram = nltk.ngrams(sentence.split(), 2)\n","trigram = nltk.ngrams(sentence.split(), 3)\n","\n","print(list(unigram))\n","print(list(bigram))\n","print(list(trigram))\n","\n","# 동일하다"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ytdf1ufbJbRP","executionInfo":{"status":"ok","timestamp":1761274935159,"user_tz":-540,"elapsed":50,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"5dc3cfdf-6c5e-47e0-9a34-0df662fd7936"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n","[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n","[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n","[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n","[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n","[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n"]}]},{"cell_type":"markdown","source":["# 2. TF-IDF\n","Term Frequency-Inverse Document Frequency"],"metadata":{"id":"ZltFfFNXKOJA"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","corpus = [\n","    \"That movie is famous movie\",\n","    \"I like that actor\",\n","    \"I don’t like that actor\"\n","]\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_vectorizer.fit(corpus)\n","tfidf_matrix = tfidf_vectorizer.transform(corpus)\n","\n","# 또는 fit_transorm 메서드로 학습과 변환 동시 수행\n","# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n","\n","print(tfidf_matrix.toarray())\n","print(tfidf_vectorizer.vocabulary_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PM0AMZ86K8lm","executionInfo":{"status":"ok","timestamp":1761276145835,"user_tz":-540,"elapsed":37,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"7aed2764-f9da-4961-8996-ac6cfdfbcacf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n","  0.2344005 ]\n"," [0.61980538 0.         0.         0.         0.61980538 0.\n","  0.48133417]\n"," [0.4804584  0.63174505 0.         0.         0.4804584  0.\n","  0.37311881]]\n","{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"]}]},{"cell_type":"markdown","source":["# 3. Word2Vec"],"metadata":{"id":"c9oqkLvsO_GL"}},{"cell_type":"markdown","source":["## 3.1. Skip-gram 모델\n"],"metadata":{"id":"e4AVVoAaQxWR"}},{"cell_type":"markdown","source":["계층적 소프트맥스나 네거티브 샘플링과 같은 효율적인 기법을 적용하지 않은 기본 형식의 skip-gram 모델.  \n","단순히 입력 단어와 주변 단어를 룩업 테이블에서 가져와서 내적을 계산하고, 손실 함수를 최소화하는 방식으로 학습된다."],"metadata":{"id":"DEyU5_g3mr2X"}},{"cell_type":"code","source":["# 기본 skip-gram 클래스\n","from torch import nn\n","\n","class VanillaSkipgram(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super().__init__()\n","        self.embedding = nn.Embedding(\n","            num_embeddings = vocab_size,\n","            embedding_dim = embedding_dim\n","        )\n","        self.linear = nn.Linear(\n","            in_features = embedding_dim,\n","            out_features = vocab_size\n","        )\n","\n","    def forward(self, input_ids):\n","        embeddings = self.embedding(input_ids)\n","        output = self.linear(embeddings)\n","        return output"],"metadata":{"id":"OWnaKa00l5XB","executionInfo":{"status":"ok","timestamp":1761397620764,"user_tz":-540,"elapsed":1763,"user":{"displayName":"최원진","userId":"05911164394888239170"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#!pip install Korpora\n","#!pip install konlpy"],"metadata":{"id":"F-9id4qJsYMI","executionInfo":{"status":"ok","timestamp":1761397634785,"user_tz":-540,"elapsed":5091,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f2b8d4d3-038d-49ca-9b62-e7efa3b4aa34"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Using cached konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Using cached jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n","Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","Using cached jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.6.0 konlpy-0.6.0\n"]}]},{"cell_type":"markdown","source":["테스트 세트를 불러오고, Okt 토크나이저로 형태소를 추출한다."],"metadata":{"id":"keHvZXgfeM-A"}},{"cell_type":"code","source":["# 영화 리뷰 데이터세트 전처리\n","import pandas as pd\n","from Korpora import Korpora\n","from konlpy.tag import Okt\n","\n","corpus = Korpora.load(\"nsmc\")\n","corpus = pd.DataFrame(corpus.test)\n","\n","tokenizer = Okt()\n","tokens = [tokenizer.morphs(review) for review in corpus.text]\n","print(tokens[:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TlQTdWLtmlAL","executionInfo":{"status":"ok","timestamp":1761397778514,"user_tz":-540,"elapsed":140520,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"d3621954-b051-4409-9f9b-c9b33cddc656"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n","    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n","\n","    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n","    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n","    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n","\n","    # Description\n","    Author : e9t@github\n","    Repository : https://github.com/e9t/nsmc\n","    References : www.lucypark.kr/docs/2015-pyconkr/#39\n","\n","    Naver sentiment movie corpus v1.0\n","    This is a movie review dataset in the Korean language.\n","    Reviews were scraped from Naver Movies.\n","\n","    The dataset construction is based on the method noted in\n","    [Large movie review dataset][^1] from Maas et al., 2011.\n","\n","    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n","\n","    # License\n","    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n","    Details in https://creativecommons.org/publicdomain/zero/1.0/\n","\n"]},{"output_type":"stream","name":"stderr","text":["[nsmc] download ratings_train.txt: 14.6MB [00:00, 118MB/s]                             \n","[nsmc] download ratings_test.txt: 4.90MB [00:00, 54.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"]}]},{"cell_type":"markdown","source":["토큰화된 데이터를 활용해 단어 사전을 구축해보자."],"metadata":{"id":"bZIPlbFPeaZg"}},{"cell_type":"code","source":["# 단어 사전 구축\n","from collections import Counter\n","\n","def build_vocab(corpus, n_vocab, special_tokens):\n","    counter = Counter()\n","    for tokens in corpus:\n","        counter.update(tokens)\n","    vocab = special_tokens\n","    for token, count in counter.most_common(n_vocab):\n","        vocab.append(token)\n","    return vocab\n","\n","vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n","token_to_id = {token: idx for idx, token in enumerate(vocab)}\n","id_to_token = {idx: token for idx, token in enumerate(vocab)}\n","\n","print(vocab[:10])\n","print(len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OV3-OYGnaxi","executionInfo":{"status":"ok","timestamp":1761397778679,"user_tz":-540,"elapsed":171,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"629bb8e5-7aa5-4e4f-f3e0-3ee909e61989"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n","5001\n"]}]},{"cell_type":"code","source":["# skip-gram의 단어 쌍 추출\n","def get_word_pairs(tokens, window_size):\n","    pairs = []\n","    for sentence in tokens:\n","        sentence_length = len(sentence)\n","        for idx, center_word in enumerate(sentence):\n","            window_start = max(0, idx - window_size)\n","            window_end = min(sentence_length, idx + window_size + 1)\n","            center_word = sentence[idx]\n","            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n","            for context_word in context_words:\n","                pairs.append([center_word, context_word])\n","    return pairs\n","\n","word_pairs = get_word_pairs(tokens, window_size=2)\n","print(word_pairs[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBZsLiI7o1TA","executionInfo":{"status":"ok","timestamp":1761283956989,"user_tz":-540,"elapsed":2401,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"aebf3725-05cb-46f5-eac3-6072692ebe50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]\n"]}]},{"cell_type":"markdown","source":["출력 결과 각 단어 쌍은 [중심 단어, 주변 단어]로 구성돼 있다.  \n","임베딩 층은 단어의 인덱스를 입력으로 받기 때문에 단어 쌍을 인덱스 쌍으로 변환해야 한다."],"metadata":{"id":"9a_q9M9xfIxD"}},{"cell_type":"code","source":["# 인덱스 쌍 변환\n","def get_index_pairs(word_pairs, token_to_id):\n","    pairs =  []\n","    unk_index = token_to_id[\"<unk>\"]\n","    for word_pair in word_pairs:\n","        center_word, context_word = word_pair\n","        center_index = token_to_id.get(center_word, unk_index)\n","        context_index = token_to_id.get(context_word, unk_index)\n","        pairs.append([center_index, context_index])\n","    return pairs\n","\n","index_pairs = get_index_pairs(word_pairs, token_to_id)\n","print(index_pairs[:5])\n","print(len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRs_a5Usp--E","executionInfo":{"status":"ok","timestamp":1761283959116,"user_tz":-540,"elapsed":2129,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"a37a3325-d47b-439b-e3dc-5a876c2c8a53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n","5001\n"]}]},{"cell_type":"markdown","source":["이렇게 생성된 인덱스 쌍은 Skip-gram 모델의 임력 데이터로 사용된다."],"metadata":{"id":"-uf2tq-6fc43"}},{"cell_type":"code","source":["# 데이터로더 적용\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","index_pairs = torch.tensor(index_pairs)\n","center_indexes = index_pairs[:, 0]\n","context_indexes = index_pairs[:, 1]\n","\n","dataset = TensorDataset(center_indexes, context_indexes)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"ZjlCcLZiq7pc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["손실함수는 단어 사전 크기만큼 클래스가 있는 분류 문제이므로 크로스 엔트로피를 사용한다.  \n","크로스 엔트로피는 내부적으로 softmax 연산을 수행하기 때문에 신경망의 출력값을 후처리 없이 활용할 수 있다."],"metadata":{"id":"QE6aF5fbfq5Y"}},{"cell_type":"code","source":["# skip-gram 모델 준비 작업\n","from torch import optim\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"],"metadata":{"id":"mQ_fYi4EsEsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","for epoch in range(10):\n","    cost = 0.0\n","    for input_ids, target_ids in dataloader:\n","        input_ids = input_ids.to(device)\n","        target_ids = target_ids.to(device)\n","\n","        logits = word2vec(input_ids)\n","        loss = criterion(logits, target_ids)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        cost += loss\n","\n","    cost = cost / len(dataloader)\n","    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-w1S_BDpsWPJ","executionInfo":{"status":"ok","timestamp":1761285009406,"user_tz":-540,"elapsed":976221,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"38503bc4-948f-4b4d-c95c-0d74000434ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch :    1, Cost : 6.198\n","Epoch :    2, Cost : 5.983\n","Epoch :    3, Cost : 5.933\n","Epoch :    4, Cost : 5.903\n","Epoch :    5, Cost : 5.881\n","Epoch :    6, Cost : 5.863\n","Epoch :    7, Cost : 5.848\n","Epoch :    8, Cost : 5.835\n","Epoch :    9, Cost : 5.823\n","Epoch :   10, Cost : 5.813\n"]}]},{"cell_type":"markdown","source":["모델이 간단한 구조이고, 데이터 수도 적은데 학습 시간이 굉장히 오래 걸린다.  \n","이럴 경우 계층적 소프트맥스나 네거티브 샘플링을 적용하면 시간을 단축할 수 있다."],"metadata":{"id":"p0CTSuAHgvYm"}},{"cell_type":"code","source":["# 임베딩 값 추출\n","token_to_embedding = dict()\n","embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n","\n","for word, embedding in zip(vocab, embedding_matrix):\n","    token_to_embedding[word] = embedding\n","\n","index = 30\n","token = vocab[index]\n","token_embedding = token_to_embedding[token]\n","\n","print(token)\n","print(token_embedding)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPWFMcputYnY","executionInfo":{"status":"ok","timestamp":1761285010000,"user_tz":-540,"elapsed":578,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"52ec7035-ed10-4e2d-bcda-fb8bbe83ae9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["연기\n","[-0.84851855 -1.8222042   0.127349   -0.8652812  -1.0437642   0.3097375\n"," -2.31448     1.0563041  -0.6822946  -0.74907726  1.2799673   0.7929172\n"," -0.5319757  -1.7590579  -0.01364881  0.98411477 -1.0765609  -0.9145792\n"," -0.60660434 -0.24022445 -0.60561025  0.87058496 -1.2078757  -0.2727345\n","  0.26137322 -1.2436544  -0.03175946 -0.1551774  -0.9634912   0.5876038\n"," -0.6240664   0.3688197  -0.01440321 -0.39620957 -0.7398726  -0.45694444\n","  0.51004624 -0.07355792 -0.13838941  0.29571134 -0.6803196  -1.2643231\n","  0.18040971 -1.5091066   0.25738162 -0.24687687 -0.66137844  1.3742735\n"," -1.059737   -0.20769632  1.9868205  -0.02364214 -0.56665576 -1.8598207\n"," -0.5447131  -0.42990255  0.63883036 -0.27989024 -1.041601   -1.402175\n","  0.6961753  -0.43975404 -0.26018244 -0.50219434  1.0543333   0.40132272\n"," -1.3169492  -0.1853549   0.24995318 -0.6179749   0.8304516  -0.5887776\n"," -1.4016972   0.15095223  0.93971765 -0.13504054  0.22650304  0.894877\n"," -1.3905934  -0.50422287  0.9724714   0.65811294  1.4445307   1.3359971\n"," -1.540832   -0.19532883  0.42378756  0.10464615 -0.5830436  -0.01613607\n"," -0.19994013  0.11660869 -0.31159624 -1.1701415  -0.66567016  0.9874459\n"," -0.9407628   0.94691646  1.235928   -0.73004353  0.9309286   0.9836324\n"," -0.22243886  1.2892963  -0.30547848 -0.5833308  -0.88439685  1.2448435\n","  0.8209246   1.1959792  -0.25556684  0.45816192 -0.40513974  0.7834556\n"," -0.46648964 -0.78036726  0.63218725  0.07741584  0.7661303   0.45329064\n"," -0.602805    1.345613   -0.17048833  1.2521799   2.3024874  -0.8223549\n"," -0.31300786  0.47334635]\n"]}]},{"cell_type":"code","source":["# 단어 임베딩 유사도 계산 (코사인 유사도)\n","import numpy as np\n","from numpy.linalg import norm\n","\n","def consine_similarity(a, b):\n","    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n","    return cosine\n","\n","def top_n_index(cosine_matrix, n):\n","    closest_indexes = cosine_matrix.argsort()[::-1]\n","    top_n = closest_indexes[1 : n + 1]\n","    return top_n\n","\n","cosine_matrix = consine_similarity(token_embedding, embedding_matrix)\n","top_n = top_n_index(cosine_matrix, n=5)\n","\n","print(f\"{token}와 가장 유사한 5 개 단어\")\n","for index in top_n:\n","    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NGbSO_xucrc","executionInfo":{"status":"ok","timestamp":1761285061496,"user_tz":-540,"elapsed":25,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"062e4cfe-e02c-4ab8-d70c-f1a09a353866"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["연기와 가장 유사한 5 개 단어\n","탄 - 유사도 : 0.3392\n","잔 - 유사도 : 0.3288\n","앙 - 유사도 : 0.2936\n","좋았는데 - 유사도 : 0.2895\n","도통 - 유사도 : 0.2875\n"]}]},{"cell_type":"markdown","source":["코사인 유사도는 두 벡터가 유사할수록 값이 1에 가까워지고, 다를수록 0에 가까워진다."],"metadata":{"id":"n2GzLpiFgQTJ"}},{"cell_type":"markdown","source":["## 3.2. Gensim 라이브러리 기반 Word2Vec 모델\n","Pytorch보다 모델 학습이 훨씬 더 빠르다."],"metadata":{"id":"wpdaG8sXvmoA"}},{"cell_type":"code","source":["#!pip install gensim"],"metadata":{"id":"31VOw9w6wEFC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from Korpora import Korpora\n","from konlpy.tag import Okt\n","\n","corpus = Korpora.load(\"nsmc\")\n","corpus = pd.DataFrame(corpus.test)\n","\n","tokenizer = Okt()\n","tokens = [tokenizer.morphs(review) for review in corpus.text]"],"metadata":{"id":"FzGL5YmTw9Vr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# word2vec 모델 학습\n","from gensim.models import Word2Vec\n","\n","word2vec = Word2Vec(\n","    sentences=tokens,\n","    vector_size=128,\n","    window=5,\n","    min_count=1,\n","    sg=1, # skip-gram모델 사용\n","    epochs=3,\n","    max_final_vocab=10000\n",")"],"metadata":{"id":"bTjQADlgwGC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 임베딩 추출 및 유사도 계산\n","word = \"연기\"\n","print(word2vec.wv[word])\n","print(word2vec.wv.most_similar(word, topn=5))\n","print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qNZwwcWSye-N","executionInfo":{"status":"ok","timestamp":1761285620577,"user_tz":-540,"elapsed":14,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"a2f4b47d-7571-4fd6-bc14-125cafc30c80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.05487183 -0.40850228  0.36768085  0.46263567 -0.087214    0.01316468\n","  0.1699383  -0.0036428  -0.5224892   0.32200712 -0.11613942 -0.19556259\n"," -0.02845111 -0.13054879 -0.29952967 -0.03536183 -0.22289719  0.55767673\n"," -0.17454194  0.5662799   0.6485446   0.63832265 -0.28808925 -0.12988247\n"," -0.10172804 -0.02290674 -0.48083398  0.13490091  0.20717323 -0.06965394\n"," -0.34435913  0.28688383  0.36697498 -0.02062768 -0.17954223 -0.3949535\n","  0.25773275 -0.2881663  -0.02794877 -0.2663027  -0.17277122  0.33320513\n"," -0.16124104 -0.41227168 -0.03408     0.18588974 -0.14622587 -0.16861063\n","  0.13592143  0.0870279   0.8002509   0.32507375  0.02429863  0.29627633\n"," -0.36664808 -0.31085148  0.2279023   0.2102413  -0.28402087  0.22580619\n"," -0.12934664 -0.06199892  0.09000939  0.19747588 -0.21389918 -0.0575014\n"," -0.09981064  0.32835373  0.08838161 -0.27330688 -0.30871361 -0.3007062\n"," -0.37315658  0.15201263 -0.23684643  0.17152165 -0.2528402  -0.40183845\n"," -0.15333372  0.40769902  0.04762038 -0.12024882  0.21434164  0.96439743\n","  0.26097134  0.2608099   0.05150462 -0.45048657  0.16014813  0.01232095\n"," -0.1458812  -0.01029946  0.12451695  0.07461096  0.29851595 -0.07191736\n","  0.15959635  0.0949199  -0.03322706 -0.40109864 -0.16917248 -0.30715272\n","  0.21267135 -0.39602092 -0.10929188 -0.12955955  0.41679752  0.23441201\n"," -0.1260605  -0.36083126  0.16054451  0.01185265 -0.15723608  0.12900148\n","  0.11429018 -0.18674627  0.39327478  0.33772522 -0.05603467  0.25893843\n","  0.07344563 -0.24702278  0.14131969  0.02035759 -0.10546984  0.03419431\n"," -0.42202064 -0.33596107]\n","[('연기력', 0.7862061858177185), ('캐스팅', 0.747041642665863), ('조연', 0.7116992473602295), ('연기자', 0.710269033908844), ('몰입도', 0.7031030654907227)]\n","0.78620625\n"]}]},{"cell_type":"markdown","source":["# 4. fastText 모델\n"],"metadata":{"id":"7865cmuNyunE"}},{"cell_type":"code","source":["# KorNLI 데이터세트 전처리\n","from Korpora import Korpora\n","\n","corpus = Korpora.load(\"kornli\")\n","corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n","tokens = [sentence.split() for sentence in corpus_texts]\n","\n","print(tokens[:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXUT9S6H0q9d","executionInfo":{"status":"ok","timestamp":1761288129321,"user_tz":-540,"elapsed":56449,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"ba72cc9c-3594-4e96-dfb7-9c88ac0764dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n","    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n","\n","    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n","    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n","    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n","\n","    # Description\n","    Author : KakaoBrain\n","    Repository : https://github.com/kakaobrain/KorNLUDatasets\n","    References :\n","        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n","           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n","           (https://arxiv.org/abs/2004.03289)\n","\n","    This is the dataset repository for our paper\n","    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n","    (https://arxiv.org/abs/2004.03289)\n","    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n","\n","    # License\n","    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n","    Details in https://creativecommons.org/licenses/by-sa/4.0/\n","\n"]},{"output_type":"stream","name":"stderr","text":["[kornli] download multinli.train.ko.tsv: 83.6MB [00:05, 16.2MB/s]                            \n","[kornli] download snli_1.0_train.ko.tsv: 78.5MB [00:00, 252MB/s]                            \n","[kornli] download xnli.dev.ko.tsv: 516kB [00:00, 32.5MB/s]\n","[kornli] download xnli.test.ko.tsv: 1.04MB [00:00, 1.65MB/s]                            \n"]},{"output_type":"stream","name":"stdout","text":["[['개념적으로', '크림', '스키밍은', '제품과', '지리라는', '두', '가지', '기본', '차원을', '가지고', '있다.'], ['시즌', '중에', '알고', '있는', '거', '알아?', '네', '레벨에서', '다음', '레벨로', '잃어버리는', '거야', '브레이브스가', '모팀을', '떠올리기로', '결정하면', '브레이브스가', '트리플', 'A에서', '한', '남자를', '떠올리기로', '결정하면', '더블', 'A가', '그를', '대신하러', '올라가고', 'A', '한', '명이', '그를', '대신하러', '올라간다.'], ['우리', '번호', '중', '하나가', '당신의', '지시를', '세밀하게', '수행할', '것이다.']]\n"]}]},{"cell_type":"code","source":["# fastText 모델 학습\n","from gensim.models import FastText\n","\n","fastText = FastText(\n","    sentences=tokens,\n","    vector_size=128,\n","    window=5,\n","    min_count=5,\n","    sg=1, # skip-gram\n","    max_final_vocab=20000,\n","    epochs=3,\n","    min_n=2,\n","    max_n=6\n",")"],"metadata":{"id":"A9hJAzD98y3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OOV 처리\n","oov_token = \"사랑해요\"\n","oov_vector = fastText.wv[oov_token]\n","\n","print(oov_token in fastText.wv.index_to_key)\n","print(fastText.wv.most_similar(oov_vector, topn=5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qy4tprne9-Qg","executionInfo":{"status":"ok","timestamp":1761288635833,"user_tz":-540,"elapsed":48,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"f2b7b615-31c7-4519-9bae-9c96ea7a0822"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","[('사랑', 0.8876461982727051), ('사랑에', 0.8189547061920166), ('사랑의', 0.7929642200469971), ('사랑을', 0.7561405897140503), ('사랑하는', 0.7555779814720154)]\n"]}]}]}