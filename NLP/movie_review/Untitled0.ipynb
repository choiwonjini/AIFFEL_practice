{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM4pLhK5QJMu1vJ4Tl04dRJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"svJspNRpUTmR","executionInfo":{"status":"ok","timestamp":1761545429790,"user_tz":-540,"elapsed":74923,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"9687bc38-11c5-414a-f677-b384f4b8b908"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim==4.3.2\n","  Downloading gensim-4.3.2.tar.gz (23.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim==4.3.2) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim==4.3.2) (1.16.2)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim==4.3.2) (7.4.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim==4.3.2) (2.0.0)\n","Building wheels for collected packages: gensim\n","  Building wheel for gensim (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gensim: filename=gensim-4.3.2-cp312-cp312-linux_x86_64.whl size=26332407 sha256=09917c32fbc7a9bfe1a4d30d8850420599eb401a76f40cee3a042bfe465665d3\n","  Stored in directory: /root/.cache/pip/wheels/50/c0/ac/7bb08954bc59d390c848b480a3fc5eec68c14bc77bf334d624\n","Successfully built gensim\n","Installing collected packages: gensim\n","Successfully installed gensim-4.3.2\n","/bin/bash: line 1: conda: command not found\n"]}],"source":["!pip install gensim==4.3.2\n","!conda install scipy==1.12.0 numpy==1.26.3 -y"]},{"cell_type":"markdown","source":["# 텍스트 데이터\n","## 1. 텍스트를 숫자로"],"metadata":{"id":"r68-CCgITJZx"}},{"cell_type":"code","source":["# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n","sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n","\n","# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n","word_list = 'i feel hungry'.split()\n","print(word_list)"],"metadata":{"id":"qBE4T7ZgUiJ6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761545429815,"user_tz":-540,"elapsed":17,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"e96a2d70-b726-4b35-b476-8abd201b913a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'feel', 'hungry']\n"]}]},{"cell_type":"markdown","source":["우리의 텍스트 데이터로부터 사전을 만들기 위해 모든 문장을 단어 단위로 쪼갠 후에 파이썬 딕셔너리(dict) 자료구조로 표현해 보겠습니다."],"metadata":{"id":"g_HRH3rJScuL"}},{"cell_type":"code","source":["index_to_word={}  # 빈 딕셔너리를 만들어서\n","\n","# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다.\n","# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다.\n","index_to_word[0]='<PAD>'  # 패딩용 단어\n","index_to_word[1]='<BOS>'  # 문장의 시작지점\n","index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n","index_to_word[3]='i'\n","index_to_word[4]='feel'\n","index_to_word[5]='hungry'\n","index_to_word[6]='eat'\n","index_to_word[7]='lunch'\n","index_to_word[8]='now'\n","index_to_word[9]='happy'\n","\n","print(index_to_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mvi6S9YHSUsA","executionInfo":{"status":"ok","timestamp":1761545429856,"user_tz":-540,"elapsed":29,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"b8328d78-304d-4327-b634-a773d4e38f0b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"]}]},{"cell_type":"code","source":["#텍스트를 숫자로 바꾸려면 위의 딕셔너리가 {텍스트:인덱스} 구조여야 합니다.\n","word_to_index={word:index for index, word in index_to_word.items()}\n","print(word_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSm-VL7CSVKH","executionInfo":{"status":"ok","timestamp":1761545483307,"user_tz":-540,"elapsed":11,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"d4df6a39-0021-4a26-b369-f9d20ac25176"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"]}]},{"cell_type":"code","source":["print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAjKOCDqSvMg","executionInfo":{"status":"ok","timestamp":1761545490757,"user_tz":-540,"elapsed":7,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"ad5e83af-5a6e-4ebe-d95c-0d2b071de37e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}]},{"cell_type":"markdown","source":["이제 우리가 가진 텍스트 데이터를 숫자로 바꿔 표현해 봅시다.\n","\n"],"metadata":{"id":"NSny8rb1SxBK"}},{"cell_type":"code","source":["# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n","# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다.\n","def get_encoded_sentence(sentence, word_to_index):\n","    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n","\n","print(get_encoded_sentence('i eat lunch', word_to_index))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOem4k42S1JX","executionInfo":{"status":"ok","timestamp":1761545512900,"user_tz":-540,"elapsed":48,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"7cb92da3-7027-48f3-8094-306c03912147"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 3, 6, 7]\n"]}]},{"cell_type":"code","source":["# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다.\n","def get_encoded_sentences(sentences, word_to_index):\n","    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n","\n","# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다.\n","encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n","print(encoded_sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzJ9VjCJS2aZ","executionInfo":{"status":"ok","timestamp":1761545557818,"user_tz":-540,"elapsed":14,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"a2ead606-39d1-4a29-d50a-6cb9a5f12787"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"]}]},{"cell_type":"code","source":["# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다.\n","def get_decoded_sentence(encoded_sentence, index_to_word):\n","    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n","\n","print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RzUm--YdTBYq","executionInfo":{"status":"ok","timestamp":1761545562888,"user_tz":-540,"elapsed":13,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"8a36d411-bf74-4399-a250-f22a29059955"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["i feel hungry\n"]}]},{"cell_type":"code","source":["# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다.\n","def get_decoded_sentences(encoded_sentences, index_to_word):\n","    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n","\n","# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n","print(get_decoded_sentences(encoded_sentences, index_to_word))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvnnoFdjTCoD","executionInfo":{"status":"ok","timestamp":1761545570653,"user_tz":-540,"elapsed":43,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"23771f4b-ecc8-4e37-9202-85d5a9593421"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['i feel hungry', 'i eat lunch', 'now i feel happy']\n"]}]},{"cell_type":"markdown","source":["## 2. Embedding 레이어\n","임베딩(Embedding)이란?  \n","자연어 처리(Natural Language Processing)분야에서 임베딩(Embedding)은 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자형태인 vector로 바꾼 결과 혹은 그 일련의 과정 전체를 의미한다. 가장 간단한 형태의 임베딩은 단어의 빈도를 그대로 벡터로 사용하는 것이다.  \n","임베딩을 통해 할수있는 것은 단어나 문장 사이의 코사인 유사도가 가장 높은 단어를 구하는 등의 계산, 단어들 사이의 의미/문법적 정보 도출 벡터 간 연산으로 단어 사이 문법적 관계 도출, 전이 학습(transfer learning)\n","임베딩은 다른 딥러닝 모델의 입력값으로 자주 쓰이고, 품질 좋은 임베딩을 쓸수록 모델의 성능이 좋아집니다."],"metadata":{"id":"-lu0Q4qbTEg-"}},{"cell_type":"code","source":["# 아래 코드는 그대로 실행하시면 에러가 발생할 것입니다.\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","\n","vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n","word_vector_dim = 4    # 4차원의 워드 벡터를 가정합니다.\n","\n","embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_vector_dim, padding_idx=0)\n","\n","# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다.\n","raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')\n","raw_inputs_tensor = torch.tensor(raw_inputs, dtype=torch.long)\n","output = embedding(raw_inputs_tensor)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":277},"id":"c3oE26LATPrI","executionInfo":{"status":"error","timestamp":1761545956655,"user_tz":-540,"elapsed":4853,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"d1609edc-eed2-44aa-ca46-be5697bbfb9a"},"execution_count":10,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3905880247.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mraw_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_encoded_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mraw_inputs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_inputs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."]}]},{"cell_type":"markdown","source":["Embedding 레이어의 인풋이 되는 문장 벡터는 그 길이가 일정해야 합니다.  \n","하지만, raw_inputs의 3개 벡터의 길이는 각각 4, 4, 5입니다."],"metadata":{"id":"OAro-PTrVIb8"}},{"cell_type":"code","source":["# 문장 벡터 뒤에 패딩(<PAD>)을 추가하여 길이를 일정하게 맞춰준다.\n","import torch.nn.functional as F\n","\n","raw_inputs = [torch.tensor(sentence, dtype=torch.long) for sentence in raw_inputs]\n","raw_inputs = torch.nn.utils.rnn.pad_sequence(raw_inputs, batch_first=True, padding_value=word_to_index['<PAD>'])\n","print(raw_inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSfJ90YhUiUA","executionInfo":{"status":"ok","timestamp":1761546157772,"user_tz":-540,"elapsed":40,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"fed270dc-5b36-4055-f025-0363b22cb440"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 3, 4, 5, 0],\n","        [1, 3, 6, 7, 0],\n","        [1, 8, 3, 4, 9]])\n"]}]},{"cell_type":"markdown","source":["짧은 문장의 뒷쪽에 0이 채워진다."],"metadata":{"id":"UoV3we0iVcnd"}},{"cell_type":"code","source":["vocab_size = len(word_to_index)   # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n","word_vector_dim = 4   # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n","\n","embedding = nn.Embedding(vocab_size, word_vector_dim, padding_idx=word_to_index['<PAD>'])\n","\n","# nn.Embedding를 통해 word vector를 모두 일정 길이로 맞춰주어야\n","# embedding 레이어의 input이 될 수 있음에 주의해 주세요.\n","encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n","maxlen = 5\n","padded_sentences = [sentence + [word_to_index['<PAD>']] * (maxlen - len(sentence)) if len(sentence) < maxlen else sentence[:maxlen] for sentence in encoded_sentences]\n","\n","raw_inputs = torch.tensor(padded_sentences, dtype=torch.long)\n","output = embedding(raw_inputs)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zz1nOj3lVgQ-","executionInfo":{"status":"ok","timestamp":1761546216726,"user_tz":-540,"elapsed":66,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"d81b8964-afb6-47ae-e6dc-59a68c79df9a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.2536, -0.4187,  0.1313,  0.7185],\n","         [-0.7690,  0.3703,  0.5953,  1.4721],\n","         [-1.3552,  0.6651,  1.5647,  0.8510],\n","         [ 1.2182,  0.4259, -0.6312, -0.4471],\n","         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n","\n","        [[-0.2536, -0.4187,  0.1313,  0.7185],\n","         [-0.7690,  0.3703,  0.5953,  1.4721],\n","         [ 0.1623, -0.9415,  1.1856, -1.0709],\n","         [ 1.4592,  0.6974,  0.3711,  0.8739],\n","         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n","\n","        [[-0.2536, -0.4187,  0.1313,  0.7185],\n","         [ 1.0676, -0.2846,  0.3480,  0.7419],\n","         [-0.7690,  0.3703,  0.5953,  1.4721],\n","         [-1.3552,  0.6651,  1.5647,  0.8510],\n","         [-0.5280,  1.2650, -1.0640, -0.7284]]], grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"markdown","source":["# RNN\n","이전 스텝의 텍스트 데이터를 처리하는 코드를 구현해보자\n"],"metadata":{"id":"2NG1Q0foViPH"}},{"cell_type":"code","source":["class Model_1(nn.Module):\n","    def __init__(self, vocab_size, word_vector_dim):\n","        super(Model_1, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, word_vector_dim)\n","        self.lstm = nn.LSTM(word_vector_dim, 8, batch_first=True)   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n","        self.fc1 = nn.Linear(8, 8)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(8, 1)    # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        x = h_n[-1]\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","vocab_size = 10   # 어휘 사전의 크기입니다(10개의 단어)\n","word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다.\n","model = Model_1(vocab_size, word_vector_dim)\n","\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJWLSBFHWctx","executionInfo":{"status":"ok","timestamp":1761548764747,"user_tz":-540,"elapsed":20,"user":{"displayName":"최원진","userId":"05911164394888239170"}},"outputId":"3c510cee-d0ac-4ade-fc19-52bac45d9184"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Model_1(\n","  (embedding): Embedding(10, 4)\n","  (lstm): LSTM(4, 8, batch_first=True)\n","  (fc1): Linear(in_features=8, out_features=8, bias=True)\n","  (relu): ReLU()\n","  (fc2): Linear(in_features=8, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ZNOktk2QWqsu"},"execution_count":null,"outputs":[]}]}